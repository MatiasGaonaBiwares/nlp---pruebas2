{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29f4498c-a511-4449-b5b5-5ecc8a59e43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-69d0281299de>:17: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "from collections import Counter    \n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from nltk.probability import FreqDist\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bef95ad5-c65d-4937-9efc-ffe08bec2ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords\n",
    "stop_words=[]\n",
    "stop_words = (pd.read_excel('C:/Users/Usuario/Desktop/Respositorio archivos .py/NLP/Datasets/stop_words_sin_tilde1.xlsx')) \n",
    "stop_words=list(stop_words['stopwords'])\n",
    "\n",
    "# new_stopwords=['dia','no','si','hábiles','15','días','respuesta','correo','electrónico','autoriza','adjunta','pantallazo','indica','tratamiento','datos','personales'\n",
    "#               'tiempo', 'gestión','1','usuario','comunica','confronta','exitoso','usuario','solicita','autorización',\n",
    "#               'ncc','nse','ncorreo','ncedula','ncedula','nnombre','noperación']\n",
    "# for i in range(0,len(new_stopwords)):\n",
    "#     stop_words.append(new_stopwords[i])\n",
    "    \n",
    "    \n",
    "eliminar_frases=['usuario se comunica','usuaria se comunica','confronta exitosa','usuario solicita información',\n",
    " 'usuaria solicita información','se informa que','se le indica correo','se realiza','se solicita','se genera certificación de pagos',\n",
    " 'se envía la solicitud al correo','por parte del usuariose envía','se genera certificación de pagos','del proyecto','se informa',\n",
    " 'se brinda','usuaria','solicita información','se informa que','usuario solicita','usuaria solicita','retroalimentacion','se toma solicitud',\n",
    " 'solicita','usuaria se comunica para manifestar inconformidad','usuario se comunica','se indica','se informa','cédula','celular','solicitud',\n",
    " 'cajero:','punto:','correo:','errada:','correc:','radicado:','nombre:','cc:','correo:','telefono:','direccion:','radicado:','os:',\n",
    " 'ps','usuario solicita','pago realizado','buenas tardes','datos personales','respuesta al correo electronico','correo electronico se indica','usuaria solicita',\n",
    " 'adjunta pantallazo','al correo sac','se lee articulo','se adjunta','pantallazo de correo','confronta no exitoso','usuario autoriza','buenas tardes',\n",
    " 'cordial saludo','correo electronico','dias habiles','buenos dias','respuesta al correo','autoriza respuesta','el dia de hoy','tiempo de gestion de','quedo atenta','muchas gracias','articulo de la ley','autoriza tiempo',\n",
    " 'ley','buen dia','pronta respuesta','cedula','numero de referencia','hasta la fecha','el nombre del titular']\n",
    "\n",
    "\n",
    "#Frases que no aportan valor y no son tenídas en cuenta\n",
    "# stop_phrases=[' _ _ ',' _ _ ''nombre_del_proyecto','descripción_del_asesor','número_de_caso','realizar_un_pago',\n",
    "# 'día_de_hoy','usuaria_informa_que','DÍA_DE_HOY','USUARIA_INFORMA_QUE','ASESORA_ELIZABETH_HERNÁNDEZ',\n",
    "# 'RADICA_ASESORA_ELIZABETH','OPERACIÓN_NOMBRE_DEL','DÍA_DE_AYER','DÍA_DE_HOY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f1d0454-e863-47a6-abea-8c50765fd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importo\n",
    "Encuestas = pd.read_excel('C:/Users/Usuario/Desktop/Efecty/Proyecto VOZ/CRM - Dinamycs PQRS/Observaciones de PQRS para frases.xlsx')\n",
    "# Vista de búsqueda avanzada de casos 29-09-2021 10-33-48.xlsx')\n",
    "\n",
    "# \n",
    "#Encuestas=Encuestas[(Encuestas['Canal de ingreso']=='Web')|(Encuestas['Canal de ingreso']=='Correo electrónico')]\n",
    "Encuestas.reset_index(inplace=True)\n",
    "Encuestas.rename(columns={'index':'index_inicial'},inplace=True)\n",
    "# Encuestas['Descripción']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2deeae06-ef09-4f37-bc36-6217b3c078a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encuestas.rename(columns={'DESCRIPCION':'Descripción'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae4b718a-e589-42b5-8a31-8f721e8d6d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encuestas['Descripción']=Encuestas['Descripción'].astype(str)\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: x.lower())\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub('se\\s[a-z]{1,10}','',str(x)))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub('á','a',str(x)))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub('é','e',str(x)))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub('í','i',str(x)))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub('ó','o',str(x)))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub('ú','u',str(x)))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub(r\"\"\"[\\n\\t]\"\"\",\" \",x))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub(\"\"\"[-*\\_¡!@#$:).;–,¿?&'_>/]{1,4}\"\"\",\"\",x))\n",
    "Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub(r\"\"\"[0-9]\"\"\",\"\",x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1861b38-d9c0-48a4-b207-f0584beeac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in eliminar_frases:\n",
    "    Encuestas['Descripción']=Encuestas['Descripción'].apply(lambda x: re.sub(f,'',str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91452be5-54b9-421e-8f19-99fbbc19e708",
   "metadata": {},
   "outputs": [],
   "source": [
    "pa_token=re.sub(\"\"\"[,']\"\"\",'',str(list(Encuestas['Descripción'])))\n",
    "tokenized_word=word_tokenize(pa_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12f3dfdf-f59b-490f-bf62-db2d76f26884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pa_token=re.sub(\"\"\"[,']\"\"\",'',str(list(Encuestas['Descripción'])))\n",
    "# tokenized_word_0=word_tokenize(pa_token)\n",
    "\n",
    "#TARDA MUUUUUCHO\n",
    "# #MISPELL. Ver de agregar las palabras de efecty\n",
    "# from autocorrect import Speller\n",
    "# spell = Speller(lang='es')\n",
    "\n",
    "# tokenized_word=[]\n",
    "# for r in tokenized_word_0:\n",
    "#     tokenized_word.append(spell(r)) #No considerar tildes (sacarlas siempre antes)\n",
    "# # diccionario propio de efecty: efecty, siwewb, TUCUENTA, Portalvirtual,trilla\n",
    "# # spell('prevalidar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49e88450-15cc-42b6-81c5-e7c3b6bd00c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qty_most_common=30\n",
    "def gen_frases(qty_words_ngram,max_stopwords,df):\n",
    "    global qty_most_common\n",
    "    global Top_words\n",
    "    Top_words=[]\n",
    "    Encuestas2=pd.DataFrame()\n",
    "    ongrams=ngrams(tokenized_word,qty_words_ngram)\n",
    "    ongrams=list(zip(ongrams))\n",
    "    ongrams=pd.DataFrame(ongrams)\n",
    "    ongrams['r']=0\n",
    "    for i in range(0,qty_words_ngram):\n",
    "        ongrams['val'+str(i)]=ongrams[0].apply(lambda x: x[i])\n",
    "        ongrams['w'+str(i)]=0\n",
    "        index_on_val=ongrams[ongrams['val'+str(i)].isin(stop_words)].index\n",
    "        ongrams.loc[index_on_val,'w'+str(i)]=1\n",
    "        ongrams['r']=ongrams['r']+ongrams['w'+str(i)]\n",
    "\n",
    "    ongrams=ongrams[(ongrams['r']<=max_stopwords)&(ongrams['w'+str(qty_words_ngram-1)]==0)]\n",
    "    ongrams_=[]\n",
    "    ongrams.reset_index(inplace=True)\n",
    "\n",
    "    for i in range(len(ongrams[0])):\n",
    "        ongrams_.append(re.sub(\"\"\"[¡!@)#($:.;,¿?&'_]\"\"\",\"\",str(ongrams[0][i]))) #Agregar nan\n",
    "\n",
    "    \n",
    "    fdist = FreqDist((ongrams_))\n",
    "    \n",
    "    df['frases']=''\n",
    "    for i in range(0,qty_most_common):\n",
    "        index_frases=df[(df['Descripción'].str.contains(str(fdist.most_common(qty_most_common)[i][0])))&(df['frases']=='')].index\n",
    "        index_frases_duplicadas=df[(df['Descripción'].str.contains(str(fdist.most_common(qty_most_common)[i][0])))&(df['frases']!='')].index\n",
    "#Si, veo que esta en el top, pero que no está en las frases df['Descripción'], es porque son 2 palabras que entre medio tenian una frase que \n",
    "# se eliminó y queda un espacio grande entre medio que te lo considera dentro del top, pero cuando lo busca no lo encuentra,\n",
    "# porque no existe tal cual, sino con un espacio de por medio y una frase sin sentido.\n",
    "        df.loc[index_frases,'frases']=fdist.most_common(qty_most_common)[i][0]\n",
    "        Top_words.append(fdist.most_common(qty_most_common)[i][0])\n",
    "        aux2=df.loc[index_frases_duplicadas,:]\n",
    "        aux2.loc[:,'frases']=fdist.most_common(qty_most_common)[i][0]\n",
    "        Encuestas2=pd.concat([aux2,Encuestas2])\n",
    "#     Encuestas2.drop(columns=['level_0'])\n",
    "    Encuestas2.reset_index(inplace=True)\n",
    "#     Encuestas2.drop(columns=['level_0'])\n",
    "    df=pd.concat([df,Encuestas2])\n",
    "    Encuestas_con_frases=df[df['frases']!='']\n",
    "#     df['frases']=df['frases'].apply(lambda x: x.upper())\n",
    "#     Top_words=fdist.most_common(qty_most_common)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84744e45-0de2-4828-bc8a-3819d5074c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRUEBAS!!\n",
    "# qty_words_ngram=2\n",
    "# max_stopwords=0\n",
    "# qty_most_common=20\n",
    "# df=Encuestas\n",
    "\n",
    "\n",
    "# global Top_words\n",
    "# Top_words=[]\n",
    "# Encuestas2=pd.DataFrame()\n",
    "# ongrams=ngrams(tokenized_word,qty_words_ngram)\n",
    "# ongrams=list(zip(ongrams))\n",
    "# ongrams=pd.DataFrame(ongrams)\n",
    "# ongrams['r']=0\n",
    "# for i in range(0,qty_words_ngram):\n",
    "#     ongrams['val'+str(i)]=ongrams[0].apply(lambda x: x[i])\n",
    "#     ongrams['w'+str(i)]=0\n",
    "#     index_on_val=ongrams[ongrams['val'+str(i)].isin(stop_words)].index\n",
    "#     ongrams.loc[index_on_val,'w'+str(i)]=1\n",
    "#     ongrams['r']=ongrams['r']+ongrams['w'+str(i)]\n",
    "\n",
    "# ongrams=ongrams[ongrams['r']<=max_stopwords]\n",
    "# ongrams_=[]\n",
    "# ongrams.reset_index(inplace=True)\n",
    "\n",
    "# for i in range(len(ongrams[0])):\n",
    "#     ongrams_.append(re.sub(\"\"\"[¡!@)#($:.;,¿?&'_]\"\"\",\"\",str(ongrams[0][i]))) #Agregar nan\n",
    "\n",
    "\n",
    "# fdist.most_common(qty_most_common)[0][0]\n",
    "# df[(df['Descripción'].str.contains(str('AUTORIZA')))]\n",
    "# df['Descripción']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d14f530-8471-4f8b-be43-d771f8836cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_frases1=gen_frases(1,0,20,Encuestas)\n",
    "# R_frases1=R_frases1[R_frases1['frases']!='']\n",
    "# top_1_palabra=pd.DataFrame(Top_words)\n",
    "\n",
    "R_frases2=gen_frases(2,0,Encuestas) \n",
    "R_frases2=R_frases2[R_frases2['frases']!='']\n",
    "top_2_palabras=pd.DataFrame(Top_words)\n",
    "\n",
    "R_frases3=gen_frases(3,1,Encuestas) \n",
    "R_frases3=R_frases3[R_frases3['frases']!='']\n",
    "top_3_palabras=pd.DataFrame(Top_words)\n",
    "\n",
    "R_frases4=gen_frases(4,2,Encuestas) \n",
    "R_frases4=R_frases4[R_frases4['frases']!='']\n",
    "top_4_palabras=pd.DataFrame(Top_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "956e871b-3fc4-4310-bd57-c6dc2f7f0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encuestas3=pd.concat([Encuestas,R_frases2])\n",
    "# Encuestas3=pd.concat([Encuestas3,R_frases2])\n",
    "Encuestas3=pd.concat([Encuestas3,R_frases3])\n",
    "Encuestas3=pd.concat([Encuestas3,R_frases4])\n",
    "Encuestas3.sort_values(by='index',inplace=True)\n",
    "# Encuestas3.drop(columns=['level_0','index'],inplace=True)\n",
    "Encuestas3.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a76e364-cfca-4624-83e4-fe1cde72f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROCESO PARA LIMIPAR FRASES SEMEJANTES DENTRO DE 4 PALABRAS. Repetir este proceso para 3 y 2 palabras\n",
    "\n",
    "eliminar=[]\n",
    "index_eliminar_2=[]\n",
    "index_eliminar_3=[]\n",
    "index_eliminar_4=[]\n",
    "top_2_palabras_n=top_2_palabras\n",
    "top_4_palabras_n=top_4_palabras\n",
    "top_3_palabras_n=top_3_palabras\n",
    "\n",
    "for j in range(0,qty_most_common): \n",
    "    \n",
    "    #Pensado para 20 palabras mas comunes de 4 palabras. Compara contra las de 4 y las de 3\n",
    "    qs=re.findall('[\\w]{1,15}',top_4_palabras_n[0][j])\n",
    "    # 4 palabras solo va a ir contra 3\n",
    "    for i in range(0,len(qs)):\n",
    "        for p1 in range(0,qty_most_common):\n",
    "            try: #Mira si hay 3 palabras segúidas repetidas dentro de los 4grams, si es asi lo elimina\n",
    "                if(((qs[i]+' '+qs[i+1]+' '+qs[i+2]) in top_4_palabras_n[0][p1])&(p1!=j)):\n",
    "                    eliminar.append(top_4_palabras_n[0][p1])\n",
    "                    top_4_palabras_n[0][p1]='-4'\n",
    "                    index_eliminar_4.append(p1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            \n",
    "            try: #Mira si hay 2 palabras segúidas repetidas dentro de los 3grams, si es asi lo elimina\n",
    "                if((qs[i]+' '+qs[i+1]) in top_3_palabras_n[0][p1]):\n",
    "                    eliminar.append(top_3_palabras_n[0][p1])\n",
    "                    index_eliminar_3.append(p1)\n",
    "                    top_3_palabras_n[0][p1]='-4'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try: #Mira si hay 2 palabras segúidas repetidas dentro de los 2grams, si es asi lo elimina\n",
    "                if((qs[i]+' '+qs[i+1]) in top_2_palabras_n[0][p1]):\n",
    "                    eliminar.append(top_2_palabras_n[0][p1])\n",
    "                    index_eliminar_2.append(p1)\n",
    "                    top_2_palabras_n[0][p1]='-4'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    #Pensado para 20 palabras mas comunes de 3 palabras, compara contra las de 3 y las de 2\n",
    "    qs3=re.findall('[\\w]{1,15}',top_3_palabras_n[0][j])\n",
    "    # 3 palabras solo va a ir contra 2\n",
    "    for i in range(0,len(qs3)):\n",
    "        for p1 in range(0,qty_most_common):\n",
    "            try: #Mira si hay 3 palabras segúidas repetidas dentro de los fourgrams, si es asi lo elimina\n",
    "                if(((qs3[i]+' '+qs3[i+1]) in top_3_palabras_n[0][p1])&(p1!=j)):\n",
    "                    eliminar.append(top_3_palabras_n[0][p1])\n",
    "                    index_eliminar_3.append(p1)\n",
    "                    top_3_palabras_n[0][p1]='-3'\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            try: #Mira si hay 2 palabras segúidas repetidas dentro de los fourgrams, si es asi lo elimina\n",
    "                if((qs3[i]+' '+qs3[i+1]) in top_2_palabras_n[0][p1]):\n",
    "                    eliminar.append(top_2_palabras_n[0][p1])\n",
    "                    index_eliminar_2.append(p1)\n",
    "                    top_2_palabras_n[0][p1]='-3'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "\n",
    "#ELIMINA LAS FRASES QUE EL PROCESO ELIGIÓ LIMPIAR\n",
    "for b in range(0,len(eliminar)):\n",
    "    index_a_eliminar=Encuestas3[Encuestas3['frases']==eliminar[b]].index\n",
    "    Encuestas3.loc[index_a_eliminar,'frases']='' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "193ccc08-4e4a-4f70-950c-f065c4574d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "Encuestas3.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1225a8fb-4282-4d15-8ec8-719151b5b58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['enviar tirilla', '', 'indica enviar', 'la respuesta la emite',\n",
       "       'la emite el ce', 'devolucion de os errada',\n",
       "       'a un proyecto errado', 'proyecto errado pensaba pagar',\n",
       "       'claro fijo', 'proyecto claro', 'correo sac',\n",
       "       'informa que por error', 'pago con referencia errada',\n",
       "       'error generaron', 'normatividad articulo', 'claro movil',\n",
       "       'relacion de giros', 'novaventa integracion', 'punto de atencion',\n",
       "       'de tradicion y libertad', 'os errada al proyecto',\n",
       "       'errada autoriza', 'la devolucion del dinero', 'punto efecty',\n",
       "       'el dia de ayer', 'la orden de servicio', 'ce envio de tirilla',\n",
       "       'giros enviados', 'tratamiento de datos', 'me dirijo a ustedes',\n",
       "       'atencion prestada', 'xd xd', 'p m', 'autoriza tratamiento',\n",
       "       'confronta exitoso', 'motivo de la llamada', 'gestion autoriza',\n",
       "       'la expide el ce', 'bajo la os indicando',\n",
       "       'tiempo de gestion respuesta', 'tirilla articulo', 'ce le envien'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Encuestas3['frases'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9bd10f1-5b27-4198-98af-8828822710d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta3= \"C:/Users/Usuario/Desktop/Respositorio archivos .py/borrador/Frases_pqrs.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "094b4e2e-cf95-4bb3-8768-ffe0cede2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import ExcelWriter\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "#Ingreso las bases a hojas al archivo\n",
    "writer = pd.ExcelWriter(ruta3, engine='openpyxl')\n",
    "Encuestas3[['index_inicial','Descripción','frases']].to_excel(writer, index=True)\n",
    "writer.save()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ced5ce37-c010-4207-980a-29b909813a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DESDE ACAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7ac236f-5ac4-4ab2-bd66-f3df4645efcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/haibaral/spanish-word2vec/notebook\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bc90c80-70b7-4f55-9ab6-062e990653c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('C:/Users/Usuario/Downloads/archive/SBW-vectors-300-min5.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e1d851a-baad-4b70-835d-690914d6f7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'ENVIAR TIRILLA', 'INDICA ENVIAR', 'NUMERO DE CEDULA',\n",
    "#        'RESPUESTA LA EMITE EL', 'LA EMITE EL CE',\n",
    "#        'PROYECTO ERRADO PENSABA PAGAR', 'A UN PROYECTO ERRADO',\n",
    "#        'DEVOLUCION DE OS ERRADA', 'CLARO FIJO', 'PROYECTO CLARO',\n",
    "#        'CORREO SAC', 'POR ERROR GENERARON', 'POR ERROR GENERARON EL',\n",
    "#        'ERROR GENERARON EL PAGO', 'INFORMA QUE POR ERROR',\n",
    "#        'QUE POR ERROR GENERARON', 'CON REFERENCIA ERRADA',\n",
    "#        'PAGO CON REFERENCIA ERRADA', 'ERROR GENERARON',\n",
    "#        'ORDEN DE SERVICIO', 'NUMERO DE REFERENCIA',\n",
    "#        'NORMATIVIDAD ARTICULO', 'TIEMPO DE RESPUESTA',\n",
    "#        'TIEMPO DE RESPUESTA DE', 'RELACION DE GIROS', 'CLARO MOVIL',\n",
    "#        'HASTA LA FECHA', 'NOVAVENTA INTEGRACION', 'PUNTO DE ATENCION',\n",
    "#        'DE TRADICION Y LIBERTAD', 'CON NUMERO DE REFERENCIA',\n",
    "#        'OS ERRADA AL PROYECTO', 'ERRADA AUTORIZA',\n",
    "#        'LA DEVOLUCION DEL DINERO', 'CEDULA DE CIUDADANIA',\n",
    "#        'GIROS ENVIADOS', 'CE ENVIO', 'TRATAMIENTO DE DATOS',\n",
    "#        'ATENCION PRESTADA', 'XD XD', 'P M', 'AUTORIZA TRATAMIENTO DE',\n",
    "#        'AUTORIZA TRATAMIENTO', 'CONFRONTA EXITOSO', 'GESTION AUTORIZA',\n",
    "#        'LA EXPIDE EL CE', 'BAJO LA OS INDICANDO',\n",
    "#        'TIEMPO DE GESTION RESPUESTA'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "820833e2-cebc-4441-9259-bc1308b80345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58988434"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('ágil')\n",
    "# model.words_closer_than('belleza','mujer') #Palabras cercanas a w1 y lejanas a w2\n",
    "# model.doesnt_match(['deporte','futbol','socer','bota'])\n",
    "model.n_similarity(['tiempo'],['ellos','no','quieren','porque','hay','mucha','espera','en','trámite'])\n",
    "\n",
    "# model.most_similar_cosmul(positive='belleza',negative='mujer'). Raro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5bb7554-33c0-4ec5-84e7-48676afd3fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement wikipediaapi\n",
      "ERROR: No matching distribution found for wikipediaapi\n"
     ]
    }
   ],
   "source": [
    "pip install wikipediaapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "845ac19b-55ac-4bcc-9c87-b0ed9d79db9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg.zip/gutenberg/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-11604113f912>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgutenberg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'austen-emma.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\util.py\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                 \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mgutenberg\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('gutenberg')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/gutenberg\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Usuario/nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Usuario\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.corpus.gutenberg.words('austen-emma.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19377d-c731-4b10-ac31-be8a5c451bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "import wikipedia \n",
    "wp = wikipedia( lang='es', version='latest')\n",
    "wp.download()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
